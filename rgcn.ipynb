{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rgcn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giuseppefutia/mlog/blob/master/rgcn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hUJCrXRIj0zN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# A step-by-step guide to build R-GCN in Pytorch"
      ]
    },
    {
      "metadata": {
        "id": "RSe5Pmn6pbdu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this notebook I describe how to develop Relational Graph Convolutional Networks (R-GCNs) of the link-prediction task within Knowledge Graphs. For more information on R-GCNs we suggest the following article:\n",
        "\n",
        "\n",
        "*   Schlichtkrull, M., Kipf, T. N., Bloem, P., van den Berg, R., Titov, I., & Welling, M. (2018, June). [Modeling relational data with graph convolutional networks](https://link.springer.com/chapter/10.1007/978-3-319-93417-4_38). In European Semantic Web Conference (pp. 593-607). Springer, Cham.\n",
        "\n",
        "I take some parts of this article in order to describe and explain the source code deployed in this notebook. The main part of this code is inspired by the Deep Graph Library (DGL - https://www.dgl.ai/), that is a python package built to ease deep learning on graph, on top of existing deep learning frameworks. \n"
      ]
    },
    {
      "metadata": {
        "id": "rjGtOjY8cD73",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Install required libraries"
      ]
    },
    {
      "metadata": {
        "id": "WfbL9l93cI49",
        "colab_type": "code",
        "outputId": "1864d9f0-5d6c-42b5-c4cf-6bc0faf43927",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install dgl\n",
        "!pip install rdflib\n",
        "!pip install torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dgl in /usr/local/lib/python3.6/dist-packages (0.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (1.14.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (1.1.0)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.6/dist-packages (from dgl) (2.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.1->dgl) (4.3.2)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.6/dist-packages (4.2.2)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.6/dist-packages (from rdflib) (0.6.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib) (2.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from isodate->rdflib) (1.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1.post2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eYO-zCwXgw1W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ]
    },
    {
      "metadata": {
        "id": "vjAD4fq_cck8",
        "colab_type": "code",
        "outputId": "e4db9087-8ab1-4a99-977d-86f279fbc602",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available(): # Try to use GPU if available\n",
        "  device = torch.device('cuda')\n",
        "\n",
        "print('Your device is ' + str(device))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your device is cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qgkie2ZaOemx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ]
    },
    {
      "metadata": {
        "id": "QceuLDaMSwti",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Most code of the Utils section is adapted from authors' implementation of RGCN link prediction:\n",
        "https://github.com/MichSchli/RelationPrediction.\n",
        "\n",
        "In the utils section are implemented functions that are useful for the following steps:\n",
        "\n",
        "\n",
        "1.   Compute the adjacency matrix of the knowledge graph and the degrees of each node.\n",
        "2.   Reduce the training time sampling the edge neighborhood.\n",
        "3.   Perform the negative sampling to further reduce the training time.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "NFXMJbFEO0Wm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Functions for Graph Sampling and Building"
      ]
    },
    {
      "metadata": {
        "id": "Bz6DvX1qOeM1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_adj_and_degrees(num_nodes, triplets):\n",
        "    \"\"\" \n",
        "    Get adjacency list and degrees of the graph\n",
        "    \n",
        "    Arguments:\n",
        "    \n",
        "    num_nodes -- (int) number of nodes in the graph\n",
        "    triplets -- (matrix) size(number of training samples, 3). Each row of the\n",
        "                matrix is in the form: [subject_id, relation_id, object_id]\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    adj_list -- (structure) size(number of nodes, node degree)\n",
        "    degrees -- (vector) size(, number of nodes). Each element of the vector is\n",
        "               filled with the degree of the node.\n",
        "    \n",
        "    \"\"\"\n",
        "    adj_list = [[] for _ in range(num_nodes)]\n",
        "    for i,triplet in enumerate(triplets):\n",
        "        adj_list[triplet[0]].append([i, triplet[2]])\n",
        "        adj_list[triplet[2]].append([i, triplet[0]])\n",
        "\n",
        "    degrees = np.array([len(a) for a in adj_list])\n",
        "    adj_list = [np.array(a) for a in adj_list]\n",
        "    return adj_list, degrees\n",
        "\n",
        "  \n",
        "def sample_edge_neighborhood(adj_list, degrees, n_triplets, sample_size):\n",
        "    \"\"\" \n",
        "    Edge neighborhood sampling to reduce graph size for training purposes\n",
        "    \n",
        "    Arguments:\n",
        "    \n",
        "    adj_list -- (structure) size(number of nodes, node degree)\n",
        "    degrees -- (vector) size(, number of nodes). Each element of the vector is\n",
        "               filled with the degree of the node\n",
        "    n_triplets -- (int) number of triples in the training data\n",
        "    sample_size -- (int) number of edges to sample in each iteration (parameter)\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    edges -- list of edge indexes as result of the sampling process\n",
        "    \"\"\"\n",
        "\n",
        "    edges = np.zeros((sample_size), dtype=np.int32)\n",
        "\n",
        "    # Initialization for sampling of nodes\n",
        "    sample_counts = np.array([d for d in degrees])\n",
        "    picked = np.array([False for _ in range(n_triplets)])\n",
        "    seen = np.array([False for _ in degrees])\n",
        "\n",
        "    for i in range(0, sample_size):\n",
        "        weights = sample_counts * seen\n",
        "\n",
        "        if np.sum(weights) == 0:\n",
        "            weights = np.ones_like(weights)\n",
        "            weights[np.where(sample_counts == 0)] = 0\n",
        "\n",
        "        probabilities = (weights) / np.sum(weights)\n",
        "        \n",
        "        # Choose vertex according to the computed probabilities\n",
        "        # The number of chosen vertices is based on the sample size parameter\n",
        "        chosen_vertex = np.random.choice(np.arange(degrees.shape[0]),\n",
        "                                         p=probabilities)\n",
        "        # Store all edges and nodes linked to the chosen vertex\n",
        "        chosen_adj_list = adj_list[chosen_vertex]\n",
        "        seen[chosen_vertex] = True\n",
        "        \n",
        "        # On the basis of the chosen vertex, randomly choose the edge and pick it up\n",
        "        # from the adjacency list and store its value in the edge number\n",
        "        chosen_edge = np.random.choice(np.arange(chosen_adj_list.shape[0]))\n",
        "        chosen_edge = chosen_adj_list[chosen_edge]\n",
        "        edge_number = chosen_edge[0]\n",
        "        \n",
        "        # If the edge has benn already picked, \n",
        "        # choose another one according to the same principles\n",
        "        while picked[edge_number]:\n",
        "            chosen_edge = np.random.choice(np.arange(chosen_adj_list.shape[0]))\n",
        "            chosen_edge = chosen_adj_list[chosen_edge]\n",
        "            edge_number = chosen_edge[0]\n",
        "        \n",
        "        edges[i] = edge_number\n",
        "        other_vertex = chosen_edge[1]\n",
        "        picked[edge_number] = True\n",
        "        sample_counts[chosen_vertex] -= 1\n",
        "        sample_counts[other_vertex] -= 1\n",
        "        seen[other_vertex] = True\n",
        "        \n",
        "        # Return all edges from the sampling process\n",
        "    return edges\n",
        "\n",
        "  \n",
        "def generate_sampled_graph_and_labels(triplets, sample_size, split_size,\n",
        "                                      num_rels, adj_list, degrees,\n",
        "                                      negative_rate):\n",
        "    \"\"\"\n",
        "    Get training graph and signals\n",
        "    First perform edge neighborhood sampling on graph, then perform negative\n",
        "    sampling to generate negative samples\n",
        "    \n",
        "    Arguments:\n",
        "    \n",
        "    triplets -- (matrix) size (num of triples in the training, 3)\n",
        "    sample_size -- (int) number of nodes to take as samples\n",
        "    split_size -- (int) parameter to split the graph from (0 to 1)\n",
        "    num_rels -- (int) number of unique relations in the training set\n",
        "    adj_list -- (list) size(number of nodes, node degree)\n",
        "    degrees -- (vector) size(, number of nodes).\n",
        "    negative_rate -- (parameter)\n",
        "    \n",
        "    Returns:\n",
        "    g -- DGL graph\n",
        "    uniq_v -- (vector) unique nodes\n",
        "    rel -- (vector) relations\n",
        "    norm -- (vector) normalized degree values of each node\n",
        "    samples -- (matrix) triple samples considering positive and negative samples\n",
        "    labels -- (vector) size(,positive + negative samples). The value of each element\n",
        "              is 1 for positive samples and 0 for negative samples.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Perform edge neighbor sampling to take a subset of edges\n",
        "    edges = sample_edge_neighborhood(adj_list, degrees, len(triplets),\n",
        "                                     sample_size)\n",
        "\n",
        "    # Relabel nodes to have consecutive node ids\n",
        "    edges = triplets[edges] \n",
        "    src, rel, dst = edges.transpose()\n",
        "    # Get unique vertices: such vertices are the result of the sampling \n",
        "    # At the end of this process, we obtain an array of new triplets:\n",
        "    # we use as node indexes the values of the array to reconstruct\n",
        "    # the original src and dst\n",
        "    uniq_v, edges = np.unique((src, dst), return_inverse=True)\n",
        "    src, dst = np.reshape(edges, (2, -1))\n",
        "    relabeled_edges = np.stack((src, rel, dst)).transpose()\n",
        "\n",
        "    # Negative sampling\n",
        "    samples, labels = negative_sampling(relabeled_edges, len(uniq_v),\n",
        "                                        negative_rate)\n",
        "\n",
        "    # Further split graph, only half of the edges will be used as graph\n",
        "    # structure, while the rest half is used as unseen positive samples\n",
        "    # The graph is splitted in the dimension split_size, picking up randomly\n",
        "    # values between 0 and sample_size. \n",
        "    # According to these random values, I create the new src, dst, and rel arrays\n",
        "    split_size = int(sample_size * split_size)\n",
        "    graph_split_ids = np.random.choice(np.arange(sample_size),\n",
        "                                       size=split_size, replace=False)\n",
        "    src = src[graph_split_ids]\n",
        "    dst = dst[graph_split_ids]\n",
        "    rel = rel[graph_split_ids]\n",
        "   \n",
        "    # Build DGL graph\n",
        "    # The number of nodes is equal to unique nodes after the sampling process\n",
        "    # The number of edges is equal to src*2, because we consider in and out edges\n",
        "    print(\"# sampled nodes: {}\".format(len(uniq_v)))\n",
        "    print(\"# sampled edges: {}\".format(len(rel) * 2))\n",
        "    g, rel, norm = build_graph_from_triplets(len(uniq_v), num_rels,\n",
        "                                             (src, rel, dst))\n",
        "    \n",
        "    return g, uniq_v, rel, norm, samples, labels\n",
        "\n",
        "  \n",
        "def comp_deg_norm(g):\n",
        "    \"\"\"\n",
        "    Apply a normalization to the values of input degree for each node\n",
        "    \n",
        "    Arguments:\n",
        "    \n",
        "    g -- DGL Graph\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    norm -- np.array of normalized degree values - 1 X num of nodes\n",
        "    \"\"\"\n",
        "    in_deg = g.in_degrees(range(g.number_of_nodes())).float().numpy()\n",
        "    norm = 1.0 / in_deg\n",
        "    # If the in_deg is 0, the result of normalization is inf (put 0 in this case)\n",
        "    norm[np.isinf(norm)] = 0\n",
        "\n",
        "    return norm\n",
        "\n",
        "  \n",
        "def build_graph_from_triplets(num_nodes, num_rels, triplets):\n",
        "    \"\"\" \n",
        "    Create a DGL graph. The graph is bidirectional because RGCN authors\n",
        "    use reversed relations. This function also generates edge type and\n",
        "    normalization factor (reciprocal of node incoming degree)\n",
        "    \n",
        "    Arguments:\n",
        "    \n",
        "    num_nodes -- Number of nodes (result of the sampling)\n",
        "    num_rels -- Number of relations (doubled compared to the original ones)\n",
        "    triplets -- Three different arrays: src, rel, dst (result of the splitted graph)\n",
        "    \n",
        "    Returns:\n",
        "    \n",
        "    g -- High-level representation of the graph created using the DGL library\n",
        "    rel -- (vector) vector of all relations\n",
        "    norm -- (vector) normalized-degree of nodes\n",
        "    \"\"\"\n",
        "    g = dgl.DGLGraph()\n",
        "    g.add_nodes(num_nodes)\n",
        "    src, rel, dst = triplets\n",
        "    \n",
        "    # The bidirectional graph is created (nodes are doubled)\n",
        "    src, dst = np.concatenate((src, dst)), np.concatenate((dst, src))\n",
        "    \n",
        "    # The relations are doubled adding the number of different relations \n",
        "    rel = np.concatenate((rel, rel + num_rels))\n",
        "    \n",
        "    # Create the edges array\n",
        "    edges = sorted(zip(dst, src, rel))    \n",
        "    dst, src, rel = np.array(edges).transpose() # 3 x Num of nodes\n",
        "    g.add_edges(src, dst)\n",
        "    \n",
        "    norm = comp_deg_norm(g)\n",
        "    print(\"# nodes: {}, # edges: {}\".format(num_nodes, len(src)))\n",
        "    \n",
        "    return g, rel, norm\n",
        "\n",
        "  \n",
        "def build_test_graph(num_nodes, num_rels, edges):\n",
        "    src, rel, dst = edges.transpose()\n",
        "    print(\"Test graph:\")\n",
        "    return build_graph_from_triplets(num_nodes, num_rels, (src, rel, dst))\n",
        "\n",
        "  \n",
        "def negative_sampling(pos_samples, num_entity, negative_rate):\n",
        "    \"\"\"\n",
        "    Apply negative sampling\n",
        "    \n",
        "    Arguments:\n",
        "    \n",
        "    pos_samples -- Relabeled edges according to the sampling process\n",
        "    num_entity -- Number of entities\n",
        "    negative_rate -- Negative rate parameter (default 10)\n",
        "    \n",
        "    Returns:\n",
        "    np.array, labels -- np.array contains a concatenation of positive samples\n",
        "                        and negative samples. \n",
        "                        labels is an array where elements are equal to 1 for \n",
        "                        positive samples and 0 for negative samples\n",
        "    \"\"\"\n",
        "    size_of_batch = len(pos_samples)\n",
        "    num_to_generate = size_of_batch * negative_rate\n",
        "    \n",
        "    # Construct an array of negative samples by repeating \n",
        "    # the positive samples for the negative_rate value.\n",
        "    # Then, I create a label array to perform the logistic regression related to\n",
        "    # the negative sampling. I initialize the first size_of_batch of labels to 1\n",
        "    # and the other values are equal to 0.\n",
        "    neg_samples = np.tile(pos_samples, (negative_rate, 1))        \n",
        "    labels = np.zeros(size_of_batch * (negative_rate + 1), dtype=np.float32)\n",
        "    labels[: size_of_batch] = 1\n",
        "    \n",
        "    # Create negative samples replacing the subject or the object according\n",
        "    # to the random probability generated in choices\n",
        "    # \n",
        "    # If the positive samples are the following: \n",
        "    # [[1540  154 2254]\n",
        "    # [ 510  193 1540]\n",
        "    # [1540   55 1269]...]\n",
        "    #\n",
        "    # The negative samples will be the following:\n",
        "    # [[1540  154 1750]\n",
        "    #  [1051  193 1540]\n",
        "    #  [85   55 1269]...]\n",
        "    #\n",
        "    # In other words we generate wrong triples\n",
        "    values = np.random.randint(num_entity, size=num_to_generate)\n",
        "    choices = np.random.uniform(size=num_to_generate)\n",
        "    subj = choices > 0.5\n",
        "    obj = choices <= 0.5\n",
        "    neg_samples[subj, 0] = values[subj]\n",
        "    neg_samples[obj, 2] = values[obj]\n",
        "    \n",
        "    return np.concatenate((pos_samples, neg_samples)), labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1FG_eBC4Phk8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Functions for Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "2hWSpJTKPiBG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sort_and_rank(score, target):\n",
        "    _, indices = torch.sort(score, dim=1, descending=True)\n",
        "    indices = torch.nonzero(indices == target.view(-1, 1))\n",
        "    indices = indices[:, 1].view(-1)\n",
        "    return indices\n",
        "\n",
        "  \n",
        "def perturb_and_get_rank(embedding, w, a, r, b, num_entity, batch_size=100):\n",
        "    \"\"\" \n",
        "    Perturb one element in the triplets\n",
        "    \n",
        "    \"\"\"\n",
        "    n_batch = (num_entity + batch_size - 1) // batch_size\n",
        "    ranks = []\n",
        "    for idx in range(n_batch):\n",
        "        print(\"batch {} / {}\".format(idx, n_batch))\n",
        "        batch_start = idx * batch_size\n",
        "        batch_end = min(num_entity, (idx + 1) * batch_size)\n",
        "        batch_a = a[batch_start: batch_end]\n",
        "        batch_r = r[batch_start: batch_end]\n",
        "        emb_ar = embedding[batch_a] * w[batch_r]\n",
        "        emb_ar = emb_ar.transpose(0, 1).unsqueeze(2) # size: D x E x 1\n",
        "        emb_c = embedding.transpose(0, 1).unsqueeze(1) # size: D x 1 x V\n",
        "        # out-prod and reduce sum\n",
        "        out_prod = torch.bmm(emb_ar, emb_c) # size D x E x V\n",
        "        score = torch.sum(out_prod, dim=0) # size E x V\n",
        "        score = torch.sigmoid(score)\n",
        "        target = b[batch_start: batch_end]\n",
        "        ranks.append(sort_and_rank(score, target))\n",
        "    return torch.cat(ranks)\n",
        "\n",
        "\n",
        "# return MRR (raw), and Hits @ (1, 3, 10)\n",
        "def evaluate(test_graph, model, test_triplets, num_entity, hits=[], eval_bz=100):\n",
        "    with torch.no_grad():\n",
        "        embedding, w = model.evaluate(test_graph)\n",
        "        s = test_triplets[:, 0]\n",
        "        r = test_triplets[:, 1]\n",
        "        o = test_triplets[:, 2]\n",
        "\n",
        "        # perturb subject\n",
        "        ranks_s = perturb_and_get_rank(embedding, w, o, r, s, num_entity, eval_bz)\n",
        "        # perturb object\n",
        "        ranks_o = perturb_and_get_rank(embedding, w, s, r, o, num_entity, eval_bz)\n",
        "\n",
        "        ranks = torch.cat([ranks_s, ranks_o])\n",
        "        ranks += 1 # change to 1-indexed\n",
        "\n",
        "        mrr = torch.mean(1.0 / ranks.float())\n",
        "        print(\"MRR (raw): {:.6f}\".format(mrr.item()))\n",
        "\n",
        "        for hit in hits:\n",
        "            avg_count = torch.mean((ranks <= hit).float())\n",
        "            print(\"Hits (raw) @ {}: {:.6f}\".format(hit, avg_count.item()))\n",
        "    return mrr.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_11LN9--BVUw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# R-GCN Layers"
      ]
    },
    {
      "metadata": {
        "id": "MC5lPUdxBagG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "I describe the implementation of a R-GCN Layer, starting from an abstract class. "
      ]
    },
    {
      "metadata": {
        "id": "fLwWlaReB8kv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RGCNLayer(nn.Module):\n",
        "  '''\n",
        "  The abstract layer of a R-GCN is built starting from the following parameters:\n",
        "  in_feat --  \n",
        "  out_feat --\n",
        "  bias --\n",
        "  activation --\n",
        "  self_loop --\n",
        "  dropout --\n",
        "  '''\n",
        "  def __init__(self, in_feat, out_feat, bias=None, activation=None,\n",
        "                 self_loop=False, dropout=0.0):\n",
        "    \n",
        "    super(RGCNLayer, self).__init__()\n",
        "        \n",
        "    self.bias = bias\n",
        "    self.activation = activation\n",
        "    self.self_loop = self_loop\n",
        "\n",
        "    if self.bias == True:\n",
        "        self.bias = nn.Parameter(torch.Tensor(out_feat))\n",
        "        nn.init.xavier_uniform_(self.bias,\n",
        "                                gain=nn.init.calculate_gain('relu'))\n",
        "\n",
        "    # weight for self loop\n",
        "    if self.self_loop:\n",
        "        self.loop_weight = nn.Parameter(torch.Tensor(in_feat, out_feat))\n",
        "        nn.init.xavier_uniform_(self.loop_weight,\n",
        "                                gain=nn.init.calculate_gain('relu'))\n",
        "\n",
        "    if dropout:\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    else:\n",
        "        self.dropout = None\n",
        "\n",
        "  # define how propagation is done in subclass\n",
        "  def propagate(self, g):\n",
        "      raise NotImplementedError\n",
        "\n",
        "  def forward(self, g):\n",
        "      if self.self_loop:\n",
        "          loop_message = torch.mm(g.ndata['h'], self.loop_weight)\n",
        "          if self.dropout is not None:\n",
        "              loop_message = self.dropout(loop_message)\n",
        "\n",
        "      self.propagate(g)\n",
        "\n",
        "      # apply bias and activation\n",
        "      node_repr = g.ndata['h']\n",
        "      if self.bias:\n",
        "          node_repr = node_repr + self.bias\n",
        "      if self.self_loop:\n",
        "          node_repr = node_repr + loop_message\n",
        "      if self.activation:\n",
        "          node_repr = self.activation(node_repr)\n",
        "\n",
        "      g.ndata['h'] = node_repr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s-ue0D5pG0nj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Tips**:\n",
        "\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "UtPxe53-LfWs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Basis Layer"
      ]
    },
    {
      "metadata": {
        "id": "gx7EA7LdLmRH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RGCNBasisLayer(RGCNLayer):\n",
        "    \n",
        "    def __init__(self, in_feat, out_feat, num_rels, num_bases=-1, bias=None,\n",
        "                 activation=None, is_input_layer=False):\n",
        "        \n",
        "        super(RGCNBasisLayer, self).__init__(in_feat, out_feat, bias, activation)\n",
        "        \n",
        "        self.in_feat = in_feat\n",
        "        self.out_feat = out_feat\n",
        "        self.num_rels = num_rels\n",
        "        self.num_bases = num_bases\n",
        "        self.is_input_layer = is_input_layer\n",
        "        if self.num_bases <= 0 or self.num_bases > self.num_rels:\n",
        "            self.num_bases = self.num_rels\n",
        "\n",
        "        # add basis weights\n",
        "        self.weight = nn.Parameter(torch.Tensor(self.num_bases, self.in_feat,\n",
        "                                                self.out_feat))\n",
        "        if self.num_bases < self.num_rels:\n",
        "            # linear combination coefficients\n",
        "            self.w_comp = nn.Parameter(torch.Tensor(self.num_rels,\n",
        "                                                    self.num_bases))\n",
        "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
        "        if self.num_bases < self.num_rels:\n",
        "            nn.init.xavier_uniform_(self.w_comp,\n",
        "                                    gain=nn.init.calculate_gain('relu'))\n",
        "\n",
        "    def propagate(self, g):\n",
        "        if self.num_bases < self.num_rels:\n",
        "            # generate all weights from bases\n",
        "            weight = self.weight.view(self.num_bases,\n",
        "                                      self.in_feat * self.out_feat)\n",
        "            weight = torch.matmul(self.w_comp, weight).view(\n",
        "                                    self.num_rels, self.in_feat, self.out_feat)\n",
        "        else:\n",
        "            weight = self.weight\n",
        "\n",
        "        if self.is_input_layer:\n",
        "            def msg_func(edges):\n",
        "                # for input layer, matrix multiply can be converted to be\n",
        "                # an embedding lookup using source node id\n",
        "                embed = weight.view(-1, self.out_feat)\n",
        "                index = edges.data['type'] * self.in_feat + edges.src['id']\n",
        "                return {'msg': embed.index_select(0, index) * edges.data['norm']}\n",
        "        else:\n",
        "            def msg_func(edges):\n",
        "                w = weight.index_select(0, edges.data['type'])\n",
        "                msg = torch.bmm(edges.src['h'].unsqueeze(1), w).squeeze()\n",
        "                msg = msg * edges.data['norm']\n",
        "                return {'msg': msg}\n",
        "\n",
        "        g.update_all(msg_func, fn.sum(msg='msg', out='h'), None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GavDRPWgNd3Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Tips**\n",
        "\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "6JXGXndBLiiE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " ## Block Layer"
      ]
    },
    {
      "metadata": {
        "id": "RB8ePcXwMP15",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RGCNBlockLayer(RGCNLayer):\n",
        "    \n",
        "    def __init__(self, in_feat, out_feat, num_rels, num_bases, bias=None,\n",
        "                 activation=None, self_loop=False, dropout=0.0):\n",
        "        \n",
        "        super(RGCNBlockLayer, self).__init__(in_feat, out_feat, bias,\n",
        "                                             activation, self_loop=self_loop,\n",
        "                                             dropout=dropout)\n",
        "        self.num_rels = num_rels\n",
        "        self.num_bases = num_bases\n",
        "        assert self.num_bases > 0\n",
        "\n",
        "        self.out_feat = out_feat\n",
        "        self.submat_in = in_feat // self.num_bases\n",
        "        self.submat_out = out_feat // self.num_bases\n",
        "\n",
        "        # assuming in_feat and out_feat are both divisible by num_bases\n",
        "        self.weight = nn.Parameter(torch.Tensor(\n",
        "            self.num_rels, self.num_bases * self.submat_in * self.submat_out))\n",
        "        nn.init.xavier_uniform_(self.weight, gain=nn.init.calculate_gain('relu'))\n",
        "\n",
        "    def msg_func(self, edges):\n",
        "        weight = self.weight.index_select(0, edges.data['type']).view(\n",
        "                    -1, self.submat_in, self.submat_out)\n",
        "        node = edges.src['h'].view(-1, 1, self.submat_in)\n",
        "        msg = torch.bmm(node, weight).view(-1, self.out_feat)\n",
        "        return {'msg': msg}\n",
        "\n",
        "    def propagate(self, g):\n",
        "        g.update_all(self.msg_func, fn.sum(msg='msg', out='h'), self.apply_func)\n",
        "\n",
        "    def apply_func(self, nodes):\n",
        "        return {'h': nodes.data['h'] * nodes.data['norm']}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EwIBPm4YNkcb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Tips**\n",
        "\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "EIgGxaxSRZRM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# R-GCN Model"
      ]
    },
    {
      "metadata": {
        "id": "smN7wF3sRgk3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BaseRGCN(nn.Module):\n",
        "    def __init__(self, num_nodes, h_dim, out_dim, num_rels, num_bases=-1,\n",
        "                 num_hidden_layers=1, dropout=0, use_cuda=False):\n",
        "        super(BaseRGCN, self).__init__()\n",
        "        self.num_nodes = num_nodes\n",
        "        self.h_dim = h_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.num_rels = num_rels\n",
        "        self.num_bases = num_bases\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.dropout = dropout\n",
        "        self.use_cuda = use_cuda\n",
        "\n",
        "        # create rgcn layers\n",
        "        self.build_model()\n",
        "\n",
        "        # create initial features\n",
        "        self.features = self.create_features()\n",
        "\n",
        "    def build_model(self):\n",
        "        self.layers = nn.ModuleList()\n",
        "        # i2h\n",
        "        i2h = self.build_input_layer()\n",
        "        if i2h is not None:\n",
        "            self.layers.append(i2h)\n",
        "        # h2h\n",
        "        for idx in range(self.num_hidden_layers):\n",
        "            h2h = self.build_hidden_layer(idx)\n",
        "            self.layers.append(h2h)\n",
        "        # h2o\n",
        "        h2o = self.build_output_layer()\n",
        "        if h2o is not None:\n",
        "            self.layers.append(h2o)\n",
        "\n",
        "    # initialize feature for each node\n",
        "    def create_features(self):\n",
        "        return None\n",
        "\n",
        "    def build_input_layer(self):\n",
        "        return None\n",
        "\n",
        "    def build_hidden_layer(self, idx):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def build_output_layer(self):\n",
        "        return None\n",
        "\n",
        "    def forward(self, g):\n",
        "        if self.features is not None:\n",
        "            g.ndata['id'] = self.features\n",
        "        for layer in self.layers:\n",
        "            layer(g)\n",
        "        return g.ndata.pop('h')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CWvu7NIgS5OE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Link prediction stage"
      ]
    },
    {
      "metadata": {
        "id": "E2ciIAQFS8Z6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch.nn.functional as F\n",
        "import dgl\n",
        "import dgl.function as fn\n",
        "from dgl.contrib.data import load_data\n",
        "\n",
        "class EmbeddingLayer(nn.Module):\n",
        "    def __init__(self, num_nodes, h_dim):\n",
        "        super(EmbeddingLayer, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(num_nodes, h_dim)\n",
        "\n",
        "    def forward(self, g):\n",
        "        node_id = g.ndata['id'].squeeze()\n",
        "        g.ndata['h'] = self.embedding(node_id)\n",
        "\n",
        "class RGCN(BaseRGCN):\n",
        "    def build_input_layer(self):\n",
        "        return EmbeddingLayer(self.num_nodes, self.h_dim)\n",
        "\n",
        "    def build_hidden_layer(self, idx):\n",
        "        act = F.relu if idx < self.num_hidden_layers - 1 else None\n",
        "        \n",
        "        return RGCNBlockLayer(self.h_dim, self.h_dim, self.num_rels, self.num_bases,\n",
        "                         activation=act, self_loop=True, dropout=self.dropout)\n",
        "\n",
        "class LinkPredict(nn.Module):\n",
        "    def __init__(self, in_dim, h_dim, num_rels, num_bases=-1,\n",
        "                 num_hidden_layers=1, dropout=0, use_cuda=False, reg_param=0):\n",
        "        super(LinkPredict, self).__init__()\n",
        "        self.rgcn = RGCN(in_dim, h_dim, h_dim, num_rels * 2, num_bases,\n",
        "                         num_hidden_layers, dropout, use_cuda)\n",
        "        self.reg_param = reg_param\n",
        "        self.w_relation = nn.Parameter(torch.Tensor(num_rels, h_dim))\n",
        "        nn.init.xavier_uniform_(self.w_relation,\n",
        "                                gain=nn.init.calculate_gain('relu'))\n",
        "\n",
        "    def calc_score(self, embedding, triplets):\n",
        "        # DistMult\n",
        "        s = embedding[triplets[:,0]]\n",
        "        r = self.w_relation[triplets[:,1]]\n",
        "        o = embedding[triplets[:,2]]\n",
        "        score = torch.sum(s * r * o, dim=1)\n",
        "        return score\n",
        "\n",
        "    def forward(self, g):\n",
        "        return self.rgcn.forward(g)\n",
        "\n",
        "    def evaluate(self, g):\n",
        "        # get embedding and relation weight without grad\n",
        "        embedding = self.forward(g)\n",
        "        return embedding, self.w_relation\n",
        "\n",
        "    def regularization_loss(self, embedding):\n",
        "        return torch.mean(embedding.pow(2)) + torch.mean(self.w_relation.pow(2))\n",
        "\n",
        "    def get_loss(self, g, triplets, labels):\n",
        "        # triplets is a list of data samples (positive and negative)\n",
        "        # each row in the triplets is a 3-tuple of (source, relation, destination)\n",
        "        embedding = self.forward(g)\n",
        "        score = self.calc_score(embedding, triplets)\n",
        "        predict_loss = F.binary_cross_entropy_with_logits(score, labels)\n",
        "        reg_loss = self.regularization_loss(embedding)\n",
        "        return predict_loss + self.reg_param * reg_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PWh7JG1YZRqW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Settings"
      ]
    },
    {
      "metadata": {
        "id": "rF02YEYKFQT5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Default values:\n",
        "\n",
        "\n",
        "```\n",
        "args = {'dropout': 0.2,\n",
        "        'n_hidden': 500,\n",
        "        'gpu': 1,\n",
        "        'lr': 1e-2,\n",
        "        'n_bases': 100,\n",
        "        'n_layers': 2,\n",
        "        'n_epochs': 6000,\n",
        "        'dataset': 'FB15k-237',\n",
        "        'eval_batch_size': 500,\n",
        "        'regularization': 0.01,\n",
        "        'grad_norm': 1.0,\n",
        "        'graph_batch_size': 30000,\n",
        "        'graph_split_size': 0.5,\n",
        "        'negative_sample': 10,\n",
        "        'evaluate_every': 500}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "sfuN9kevZ7Lx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "args = {'dropout': 0.2,\n",
        "        'n_hidden': 500,\n",
        "        'gpu': 1,\n",
        "        'lr': 1e-2,\n",
        "        'n_bases': 100,\n",
        "        'n_layers': 2,\n",
        "        'n_epochs': 10,\n",
        "        'dataset': 'FB15k-237',\n",
        "        'eval_batch_size': 500,\n",
        "        'regularization': 0.01,\n",
        "        'grad_norm': 1.0,\n",
        "        'graph_batch_size': 3000,\n",
        "        'graph_split_size': 0.5,\n",
        "        'negative_sample': 10,\n",
        "        'evaluate_every': 10}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wi7zJ-75Tme_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Scripts for running"
      ]
    },
    {
      "metadata": {
        "id": "lH9q4lKuTojg",
        "colab_type": "code",
        "outputId": "674f2fdd-4aba-4180-9f37-846cc3884bee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4650
        }
      },
      "cell_type": "code",
      "source": [
        "# load graph data\n",
        "data = load_data(args['dataset'])\n",
        "num_nodes = data.num_nodes\n",
        "train_data = data.train\n",
        "valid_data = data.valid\n",
        "test_data = data.test\n",
        "num_rels = data.num_rels\n",
        "\n",
        "# check cuda\n",
        "use_cuda = args['gpu'] >= 0 and torch.cuda.is_available()\n",
        "if use_cuda:\n",
        "    torch.device('cuda')\n",
        "\n",
        "# create model\n",
        "model = LinkPredict(num_nodes,\n",
        "                    args['n_hidden'],\n",
        "                    num_rels,\n",
        "                    num_bases=args['n_bases'],\n",
        "                    num_hidden_layers=args['n_layers'],\n",
        "                    dropout=args['dropout'],\n",
        "                    use_cuda=use_cuda,\n",
        "                    reg_param=args['regularization'])\n",
        "\n",
        "# validation and testing triplets\n",
        "valid_data = torch.LongTensor(valid_data)\n",
        "test_data = torch.LongTensor(test_data)\n",
        "\n",
        "# build test graph\n",
        "test_graph, test_rel, test_norm = build_test_graph(\n",
        "    num_nodes, num_rels, train_data)\n",
        "test_deg = test_graph.in_degrees(\n",
        "            range(test_graph.number_of_nodes())).float().view(-1,1)\n",
        "test_node_id = torch.arange(0, num_nodes, dtype=torch.long).view(-1, 1)\n",
        "test_rel = torch.from_numpy(test_rel).view(-1, 1)\n",
        "test_norm = torch.from_numpy(test_norm).view(-1, 1)\n",
        "test_graph.ndata.update({'id': test_node_id, 'norm': test_norm})\n",
        "test_graph.edata['type'] = test_rel\n",
        "\n",
        "if use_cuda:\n",
        "    model.cuda()\n",
        "\n",
        "# build adj list and calculate degrees for sampling\n",
        "adj_list, degrees = get_adj_and_degrees(num_nodes, train_data)\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "\n",
        "model_state_file = 'model_state.pth'\n",
        "forward_time = []\n",
        "backward_time = []\n",
        "loss_values = []\n",
        "train_start = time.time()\n",
        "\n",
        "# training loop\n",
        "print(\"start training...\")\n",
        "\n",
        "epoch = 0\n",
        "best_mrr = 0\n",
        "while True:\n",
        "    model.train()\n",
        "    epoch += 1\n",
        "\n",
        "    # perform edge neighborhood sampling to generate training graph and data\n",
        "    g, node_id, edge_type, node_norm, data, labels = \\\n",
        "        generate_sampled_graph_and_labels(\n",
        "            train_data, args['graph_batch_size'], args['graph_split_size'],\n",
        "            num_rels, adj_list, degrees, args['negative_sample'])\n",
        "    print(\"Done edge sampling\")\n",
        "\n",
        "    # set node/edge feature\n",
        "    node_id = torch.from_numpy(node_id).view(-1, 1)\n",
        "    edge_type = torch.from_numpy(edge_type)\n",
        "    node_norm = torch.from_numpy(node_norm).view(-1, 1)\n",
        "    data, labels = torch.from_numpy(data), torch.from_numpy(labels)\n",
        "    deg = g.in_degrees(range(g.number_of_nodes())).float().view(-1, 1)\n",
        "    if use_cuda:\n",
        "        node_id, deg = node_id.cuda(), deg.cuda()\n",
        "        edge_type, node_norm = edge_type.cuda(), node_norm.cuda()\n",
        "        data, labels = data.cuda(), labels.cuda()\n",
        "    g.ndata.update({'id': node_id, 'norm': node_norm})\n",
        "    g.edata['type'] = edge_type\n",
        "\n",
        "    t0 = time.time()\n",
        "    loss = model.get_loss(g, data, labels)\n",
        "    t1 = time.time()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), args['grad_norm']) # clip gradients\n",
        "    optimizer.step()\n",
        "    t2 = time.time()\n",
        "\n",
        "    forward_time.append(t1 - t0)\n",
        "    backward_time.append(t2 - t1)\n",
        "    loss_values.append(loss.item())\n",
        "    print(\"Epoch {:04d} | Loss {:.4f} | Best MRR {:.4f} | Forward {:.4f}s | Backward {:.4f}s\".\n",
        "          format(epoch, loss.item(), best_mrr, forward_time[-1], backward_time[-1]))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # validation\n",
        "    if epoch % args['evaluate_every'] == 0:\n",
        "        # perform validation on CPU because full graph is too large\n",
        "        if use_cuda:\n",
        "            model.cpu()\n",
        "        model.eval()\n",
        "        print(\"start eval\")\n",
        "        mrr = evaluate(test_graph, model, valid_data, num_nodes,\n",
        "                             hits=[1, 3, 10], eval_bz=args['eval_batch_size'])\n",
        "        # save best model\n",
        "        if mrr < best_mrr:\n",
        "            if epoch >= args['n_epochs']:\n",
        "                break\n",
        "        else:\n",
        "            best_mrr = mrr\n",
        "            torch.save({'state_dict': model.state_dict(), 'epoch': epoch},\n",
        "                       model_state_file)\n",
        "        if use_cuda:\n",
        "            model.cuda()\n",
        "\n",
        "print(\"training done\")\n",
        "train_done = time.time()\n",
        "print(\"Mean forward time: {:4f}s\".format(np.mean(forward_time)))\n",
        "print(\"Mean Backward time: {:4f}s\".format(np.mean(backward_time)))\n",
        "print(\"Training time: {:4f}min\").format((train_done - train_start)/60)\n",
        "\n",
        "print(\"\\nstart testing:\")\n",
        "# use best model checkpoint\n",
        "checkpoint = torch.load(model_state_file)\n",
        "if use_cuda:\n",
        "    model.cpu() # test on CPU\n",
        "model.eval()\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "print(\"Using best epoch: {}\".format(checkpoint['epoch']))\n",
        "evaluate(test_graph, model, test_data, num_nodes, hits=[1, 3, 10],\n",
        "               eval_bz=args['eval_batch_size'])\n",
        "\n",
        "# Print loss values\n",
        "plt.plot(loss_values)\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# entities: 14541\n",
            "# relations: 237\n",
            "# edges: 272115\n",
            "Test graph:\n",
            "g._in_degrees\n",
            "<bound method DGLBaseGraph.in_degrees of DGLGraph(num_nodes=14541, num_edges=544230,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})>\n",
            "g.number_of_nodes\n",
            "14541\n",
            "in_deg\n",
            "[ 10.  20. 245. ...  21.  34.  35.]\n",
            "14541\n",
            "norm\n",
            "[0.1        0.05       0.00408163 ... 0.04761905 0.02941176 0.02857143]\n",
            "# nodes: 14541, # edges: 544230\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:171: RuntimeWarning: divide by zero encountered in true_divide\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "start training...\n",
            "# sampled nodes: 2268\n",
            "# sampled edges: 3000\n",
            "g._in_degrees\n",
            "<bound method DGLBaseGraph.in_degrees of DGLGraph(num_nodes=2268, num_edges=3000,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})>\n",
            "g.number_of_nodes\n",
            "2268\n",
            "in_deg\n",
            "[3. 1. 0. ... 0. 1. 1.]\n",
            "2268\n",
            "norm\n",
            "[0.33333334 1.         0.         ... 0.         1.         1.        ]\n",
            "# nodes: 2268, # edges: 3000\n",
            "Done edge sampling\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/dgl/base.py:18: UserWarning: Initializer is not set. Use zero initializer instead. To suppress this warning, use `set_initializer` to explicitly specify which initializer to use.\n",
            "  warnings.warn(msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0001 | Loss 2.2153 | Best MRR 0.0000 | Forward 0.0257s | Backward 0.3210s\n",
            "# sampled nodes: 2182\n",
            "# sampled edges: 3000\n",
            "g._in_degrees\n",
            "<bound method DGLBaseGraph.in_degrees of DGLGraph(num_nodes=2182, num_edges=3000,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})>\n",
            "g.number_of_nodes\n",
            "2182\n",
            "in_deg\n",
            "[2. 3. 0. ... 0. 1. 1.]\n",
            "2182\n",
            "norm\n",
            "[0.5        0.33333334 0.         ... 0.         1.         1.        ]\n",
            "# nodes: 2182, # edges: 3000\n",
            "Done edge sampling\n",
            "Epoch 0002 | Loss 3.8559 | Best MRR 0.0000 | Forward 0.0107s | Backward 0.2823s\n",
            "# sampled nodes: 2294\n",
            "# sampled edges: 3000\n",
            "g._in_degrees\n",
            "<bound method DGLBaseGraph.in_degrees of DGLGraph(num_nodes=2294, num_edges=3000,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})>\n",
            "g.number_of_nodes\n",
            "2294\n",
            "in_deg\n",
            "[1. 0. 2. ... 1. 0. 0.]\n",
            "2294\n",
            "norm\n",
            "[1.  0.  0.5 ... 1.  0.  0. ]\n",
            "# nodes: 2294, # edges: 3000\n",
            "Done edge sampling\n",
            "Epoch 0003 | Loss 5.8978 | Best MRR 0.0000 | Forward 0.0100s | Backward 0.2703s\n",
            "# sampled nodes: 2285\n",
            "# sampled edges: 3000\n",
            "g._in_degrees\n",
            "<bound method DGLBaseGraph.in_degrees of DGLGraph(num_nodes=2285, num_edges=3000,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})>\n",
            "g.number_of_nodes\n",
            "2285\n",
            "in_deg\n",
            "[2. 2. 2. ... 1. 4. 1.]\n",
            "2285\n",
            "norm\n",
            "[0.5  0.5  0.5  ... 1.   0.25 1.  ]\n",
            "# nodes: 2285, # edges: 3000\n",
            "Done edge sampling\n",
            "Epoch 0004 | Loss 10.2963 | Best MRR 0.0000 | Forward 0.0098s | Backward 0.2746s\n",
            "# sampled nodes: 2161\n",
            "# sampled edges: 3000\n",
            "g._in_degrees\n",
            "<bound method DGLBaseGraph.in_degrees of DGLGraph(num_nodes=2161, num_edges=3000,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})>\n",
            "g.number_of_nodes\n",
            "2161\n",
            "in_deg\n",
            "[1. 0. 1. ... 0. 2. 1.]\n",
            "2161\n",
            "norm\n",
            "[1.  0.  1.  ... 0.  0.5 1. ]\n",
            "# nodes: 2161, # edges: 3000\n",
            "Done edge sampling\n",
            "Epoch 0005 | Loss 13.0775 | Best MRR 0.0000 | Forward 0.0101s | Backward 0.2722s\n",
            "# sampled nodes: 2298\n",
            "# sampled edges: 3000\n",
            "g._in_degrees\n",
            "<bound method DGLBaseGraph.in_degrees of DGLGraph(num_nodes=2298, num_edges=3000,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})>\n",
            "g.number_of_nodes\n",
            "2298\n",
            "in_deg\n",
            "[1. 4. 0. ... 0. 2. 0.]\n",
            "2298\n",
            "norm\n",
            "[1.   0.25 0.   ... 0.   0.5  0.  ]\n",
            "# nodes: 2298, # edges: 3000\n",
            "Done edge sampling\n",
            "Epoch 0006 | Loss 8.6645 | Best MRR 0.0000 | Forward 0.0102s | Backward 0.2732s\n",
            "# sampled nodes: 2329\n",
            "# sampled edges: 3000\n",
            "g._in_degrees\n",
            "<bound method DGLBaseGraph.in_degrees of DGLGraph(num_nodes=2329, num_edges=3000,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})>\n",
            "g.number_of_nodes\n",
            "2329\n",
            "in_deg\n",
            "[12.  1.  0. ...  2.  0.  0.]\n",
            "2329\n",
            "norm\n",
            "[0.08333334 1.         0.         ... 0.5        0.         0.        ]\n",
            "# nodes: 2329, # edges: 3000\n",
            "Done edge sampling\n",
            "Epoch 0007 | Loss 8.8041 | Best MRR 0.0000 | Forward 0.0107s | Backward 0.2716s\n",
            "# sampled nodes: 2306\n",
            "# sampled edges: 3000\n",
            "g._in_degrees\n",
            "<bound method DGLBaseGraph.in_degrees of DGLGraph(num_nodes=2306, num_edges=3000,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})>\n",
            "g.number_of_nodes\n",
            "2306\n",
            "in_deg\n",
            "[5. 0. 1. ... 1. 1. 1.]\n",
            "2306\n",
            "norm\n",
            "[0.2 0.  1.  ... 1.  1.  1. ]\n",
            "# nodes: 2306, # edges: 3000\n",
            "Done edge sampling\n",
            "Epoch 0008 | Loss 5.9480 | Best MRR 0.0000 | Forward 0.0108s | Backward 0.2731s\n",
            "# sampled nodes: 2360\n",
            "# sampled edges: 3000\n",
            "g._in_degrees\n",
            "<bound method DGLBaseGraph.in_degrees of DGLGraph(num_nodes=2360, num_edges=3000,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})>\n",
            "g.number_of_nodes\n",
            "2360\n",
            "in_deg\n",
            "[0. 1. 1. ... 2. 1. 0.]\n",
            "2360\n",
            "norm\n",
            "[0.  1.  1.  ... 0.5 1.  0. ]\n",
            "# nodes: 2360, # edges: 3000\n",
            "Done edge sampling\n",
            "Epoch 0009 | Loss 5.2608 | Best MRR 0.0000 | Forward 0.0102s | Backward 0.2730s\n",
            "# sampled nodes: 2343\n",
            "# sampled edges: 3000\n",
            "g._in_degrees\n",
            "<bound method DGLBaseGraph.in_degrees of DGLGraph(num_nodes=2343, num_edges=3000,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})>\n",
            "g.number_of_nodes\n",
            "2343\n",
            "in_deg\n",
            "[1. 1. 2. ... 0. 0. 1.]\n",
            "2343\n",
            "norm\n",
            "[1.  1.  0.5 ... 0.  0.  1. ]\n",
            "# nodes: 2343, # edges: 3000\n",
            "Done edge sampling\n",
            "Epoch 0010 | Loss 4.6824 | Best MRR 0.0000 | Forward 0.0101s | Backward 0.2723s\n",
            "start eval\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-099c7996c79b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"start eval\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         mrr = evaluate(test_graph, model, valid_data, num_nodes,\n\u001b[0;32m--> 105\u001b[0;31m                              hits=[1, 3, 10], eval_bz=args['eval_batch_size'])\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;31m# save best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmrr\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_mrr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-561786e25c76>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(test_graph, model, test_triplets, num_entity, hits, eval_bz)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_triplets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_entity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_bz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_triplets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_triplets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-057066605fe0>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, g)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# get embedding and relation weight without grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_relation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-057066605fe0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrgcn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-71f9316b438b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-020bb8c11e5a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g)\u001b[0m\n\u001b[1;32m     46\u001b[0m               \u001b[0mloop_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0;31m# apply bias and activation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-926b2534d573>\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(self, g)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'msg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dgl/graph.py\u001b[0m in \u001b[0;36mupdate_all\u001b[0;34m(self, message_func, reduce_func, apply_node_func)\u001b[0m\n\u001b[1;32m   2549\u001b[0m                                           \u001b[0mreduce_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m                                           apply_func=apply_node_func)\n\u001b[0;32m-> 2551\u001b[0;31m             \u001b[0mRuntime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m     def prop_nodes(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dgl/runtime/runtime.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(prog)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mexe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;31m#prog.pprint_exe(exe)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mexe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dgl/runtime/ir/executor.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0medge_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfdedge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mdst_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfddst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mudf_ret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mudf_ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/dgl/runtime/scheduler.py\u001b[0m in \u001b[0;36m_mfunc_wrapper\u001b[0;34m(src_data, edge_data, dst_data)\u001b[0m\n\u001b[1;32m    831\u001b[0m         ebatch = EdgeBatch(graph, (u.data, v.data, eid.data),\n\u001b[1;32m    832\u001b[0m                            src_data, edge_data, dst_data)\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mebatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m     \u001b[0m_mfunc_wrapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFUNC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_mfunc_wrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEDGE_UDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_mfunc_wrapper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdedge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfddst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-926b2534d573>\u001b[0m in \u001b[0;36mmsg_func\u001b[0;34m(self, edges)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmsg_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         weight = self.weight.index_select(0, edges.data['type']).view(\n\u001b[0m\u001b[1;32m     24\u001b[0m                     -1, self.submat_in, self.submat_out)\n\u001b[1;32m     25\u001b[0m         \u001b[0mnode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'h'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmat_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 3: Index is supposed to be 1-dimensional at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:164"
          ]
        }
      ]
    }
  ]
}