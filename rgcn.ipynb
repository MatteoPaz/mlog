{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rgcn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giuseppefutia/ml/blob/master/rgcn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "hUJCrXRIj0zN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ]
    },
    {
      "metadata": {
        "id": "RSe5Pmn6pbdu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "eYO-zCwXgw1W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Set up the system"
      ]
    },
    {
      "metadata": {
        "id": "vjAD4fq_cck8",
        "colab_type": "code",
        "outputId": "9cbc447b-ca4f-4c46-b0be-0d04bcbf80a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "if torch.cuda.is_available(): # Try to use GPU if available\n",
        "  device = torch.device('cuda')\n",
        "\n",
        "print('Your device is ' + str(device))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your device is cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GKpF5-lgg3s9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import external files\n"
      ]
    },
    {
      "metadata": {
        "id": "kZqDQoOzg_GA",
        "colab_type": "code",
        "outputId": "238e78e2-8cd7-472a-9f82-823c0400daf4",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 611
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a5671253-c686-4414-8ab2-df16052f0e33\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-a5671253-c686-4414-8ab2-df16052f0e33\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving __init.py__ to __init.py__\n",
            "Saving data_generator.py to data_generator.py\n",
            "Saving data_importer.py to data_importer.py\n",
            "Saving data_reader.py to data_reader.py\n",
            "Saving entities.dict to entities.dict\n",
            "Saving relations.dict to relations.dict\n",
            "Saving settings_importer.py to settings_importer.py\n",
            "Saving settings.json to settings.json\n",
            "Saving test.txt to test.txt\n",
            "Saving train.txt to train.txt\n",
            "Saving valid.txt to valid.txt\n",
            "User uploaded file \"__init.py__\" with length 2 bytes\n",
            "User uploaded file \"data_generator.py\" with length 1953 bytes\n",
            "User uploaded file \"data_importer.py\" with length 947 bytes\n",
            "User uploaded file \"data_reader.py\" with length 923 bytes\n",
            "User uploaded file \"entities.dict\" with length 749 bytes\n",
            "User uploaded file \"relations.dict\" with length 1543 bytes\n",
            "User uploaded file \"settings_importer.py\" with length 1048 bytes\n",
            "User uploaded file \"settings.json\" with length 1291 bytes\n",
            "User uploaded file \"test.txt\" with length 775 bytes\n",
            "User uploaded file \"train.txt\" with length 805 bytes\n",
            "User uploaded file \"valid.txt\" with length 758 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B2kax0COchxq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Prepare data"
      ]
    },
    {
      "metadata": {
        "id": "jEqToLydncEq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The preparation of data consist in the following steps:\n",
        "\n",
        "1.   TODO\n",
        "2.   TODO\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "jmmR-LvxdaMe",
        "colab_type": "code",
        "outputId": "4f459efa-95b1-45d5-d007-257b92b2583a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        }
      },
      "cell_type": "code",
      "source": [
        "from data_reader import *\n",
        "\n",
        "relations_path = 'relations.dict'\n",
        "entities_path = 'entities.dict'\n",
        "train_path = 'train.txt'\n",
        "test_path = 'test.txt'\n",
        "valid_path = 'valid.txt'\n",
        "\n",
        "# Create a list of triples using their id\n",
        "train = read_triplets_as_list(train_path, entities_path, relations_path)\n",
        "test = read_triplets_as_list(test_path, entities_path, relations_path)\n",
        "valid = read_triplets_as_list(valid_path, entities_path, relations_path)\n",
        "\n",
        "# Create a numpy matrix using the previous list\n",
        "train = np.array(train)\n",
        "test = np.array(test)\n",
        "valid = np.array(valid)\n",
        "\n",
        "# Create a pytorch tensor from numpy matrix\n",
        "train = torch.from_numpy(train)\n",
        "test  = torch.from_numpy(test)\n",
        "valid = torch.from_numpy(valid)\n",
        "\n",
        "# Read entities and relations as dictionary\n",
        "entities = read_dictionary(entities_path)\n",
        "relations = read_dictionary(relations_path)\n",
        "\n",
        "print('Data for training: ')\n",
        "print(train)\n",
        "\n",
        "print()\n",
        "\n",
        "print('Data for testing: ')\n",
        "print(test)\n",
        "\n",
        "print()\n",
        "\n",
        "print('Data for validation: ')\n",
        "print(valid)\n",
        "\n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data for training: \n",
            "tensor([[19,  8, 20],\n",
            "        [21,  9, 22],\n",
            "        [23, 10, 24],\n",
            "        [25, 11, 26],\n",
            "        [27, 12, 28],\n",
            "        [29, 13, 30],\n",
            "        [31, 14, 32],\n",
            "        [33, 15, 34],\n",
            "        [35, 16, 36],\n",
            "        [37,  7, 38]])\n",
            "\n",
            "Data for testing: \n",
            "tensor([[ 0,  0,  1],\n",
            "        [ 2,  1,  3],\n",
            "        [ 4,  2,  5],\n",
            "        [ 6,  3,  7],\n",
            "        [ 8,  0,  9],\n",
            "        [10,  4, 11],\n",
            "        [12,  5, 13],\n",
            "        [14,  6, 15],\n",
            "        [14,  6, 16],\n",
            "        [17,  7, 18]])\n",
            "\n",
            "Data for validation: \n",
            "tensor([[39, 17, 40],\n",
            "        [41, 18, 42],\n",
            "        [43, 11, 44],\n",
            "        [45, 19, 46],\n",
            "        [47, 20, 48],\n",
            "        [49, 21, 50],\n",
            "        [51,  4, 52],\n",
            "        [53, 22, 54],\n",
            "        [55, 23, 56],\n",
            "        [57, 24, 58]])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OZYB6gDYrCI0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import settings"
      ]
    },
    {
      "metadata": {
        "id": "GeqnfUUOrE_r",
        "colab_type": "code",
        "outputId": "d2e6131d-46bd-42ec-b305-667a9c078f2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "from settings_importer import *\n",
        "\n",
        "with open('settings.json') as f:\n",
        "    settings= json.load(f)\n",
        "\n",
        "encoder_settings, \\\n",
        "decoder_settings, \\\n",
        "optimizer_settings, \\\n",
        "evaluation_settings, \\\n",
        "general_settings = import_settings(settings, entities, relations, train)\n",
        "\n",
        "print('Neural Network settings: ')\n",
        "print()\n",
        "print('Encoder settings: ')\n",
        "print(encoder_settings)\n",
        "print()\n",
        "print('Decoder settings: ')\n",
        "print(decoder_settings)\n",
        "print()\n",
        "print('Optimizer settings: ')\n",
        "print(optimizer_settings)\n",
        "print()\n",
        "print('Evaluation settings: ')\n",
        "print(evaluation_settings)\n",
        "print()\n",
        "print('General settings: ')\n",
        "print(general_settings)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neural Network settings: \n",
            "\n",
            "Encoder settings: \n",
            "{'Name': 'gcn_basis', 'DropoutKeepProbability': 0.8, 'InternalEncoderDimension': 500, 'NumberOfBasisFunctions': 5, 'NumberOfLayers': 2, 'UseInputTransform': 'Yes', 'UseOutputTransform': 'No', 'AddDiagonal': 'No', 'DiagonalCoefficients': 'No', 'SkipConnections': 'No', 'StoreEdgeData': 'No', 'RandomInput': 'No', 'PartiallyRandomInput': 'No', 'Concatenation': 'No', 'NegativeSampleRate': 10, 'GraphSplitSize': 0.5, 'ExperimentName': 'models/GcnBlock', 'GraphBatchSize': 3, 'EntityCount': 59, 'RelationCount': 25, 'EdgeCount': 10, 'CodeDimension': 500}\n",
            "\n",
            "Decoder settings: \n",
            "{'Name': 'bilinear-diag', 'RegularizationParameter': 0.01, 'NegativeSampleRate': 10, 'GraphSplitSize': 0.5, 'ExperimentName': 'models/GcnBlock', 'GraphBatchSize': 3, 'EntityCount': 59, 'RelationCount': 25, 'EdgeCount': 10, 'CodeDimension': 500}\n",
            "\n",
            "Optimizer settings: \n",
            "{'MaxGradientNorm': 1, 'ReportTrainLossEvery': 100, 'EarlyStopping': {'CheckEvery': 100, 'BurninPhaseDuration': 6000}, 'Algorithm': {'Name': 'Adam', 'learning_rate': 0.01}, 'NegativeSampleRate': 10, 'GraphSplitSize': 0.5, 'ExperimentName': 'models/GcnBlock', 'GraphBatchSize': 3, 'EntityCount': 59, 'RelationCount': 25, 'EdgeCount': 10}\n",
            "\n",
            "Evaluation settings: \n",
            "{'Metric': 'MRR', 'NegativeSampleRate': 10, 'GraphSplitSize': 0.5, 'ExperimentName': 'models/GcnBlock', 'GraphBatchSize': 3, 'EntityCount': 59, 'RelationCount': 25, 'EdgeCount': 10}\n",
            "\n",
            "General settings\n",
            "{'NegativeSampleRate': 10, 'GraphSplitSize': 0.5, 'ExperimentName': 'models/GcnBlock', 'GraphBatchSize': 3, 'EntityCount': 59, 'RelationCount': 25, 'EdgeCount': 10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UQyAbPnepWdT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Build the Model"
      ]
    },
    {
      "metadata": {
        "id": "U6z39S24shgm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The Graph Convolutional Network in our case is an encoder."
      ]
    },
    {
      "metadata": {
        "id": "it2OukfatKS4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Help Functions for weight initialization"
      ]
    },
    {
      "metadata": {
        "id": "0I60aKSSlruN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The following functions are useful for the correct initialization of weights.\n",
        "\n",
        "### Glorot initialization\n",
        "\n",
        "The Glorot initialization, for instance, makes sure the weights are ‘just right’, keeping the signal in a reasonable range of values through many layers. Glorot initialization automatically determines the scale of initialization based on the number of input and output neurons.\n",
        "\n",
        "A useful explanation is available at: http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "KohNRDU8tU0D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def glorot_variance(shape):\n",
        "    return 3 / np.sqrt(shape[0] + shape[1])\n",
        "\n",
        "\n",
        "def make_weight(mean, variance, shape, init=\"normal\"):\n",
        "    if init == \"normal\":\n",
        "        initializer = np.random.normal(mean, variance, size=shape).astype(np.float32)\n",
        "    elif init == \"uniform\":\n",
        "        initializer = np.random.uniform(mean, variance, size=shape).astype(np.float32)\n",
        "\n",
        "    return torch.from_numpy(initializer)\n",
        "\n",
        "\n",
        "def make_bias(shape, init=0):\n",
        "    if init == 0:\n",
        "        return torch.from_numpy(np.zeros(shape).astype(np.float32))\n",
        "    elif init == 1:\n",
        "        return torch.from_numpy(np.ones(shape).astype(np.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oI_Fd0x8s0ZJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The RGCN model takes as input the encoder settings that contain all parameters to define the weight matrices of the encoder component."
      ]
    },
    {
      "metadata": {
        "id": "ZLIoIehMpYVW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RGCN(torch.nn.Module):\n",
        "  \n",
        "  def __init__(self, encoder):\n",
        "    \n",
        "    super(RGCN, self).__init__()\n",
        "    \n",
        "    # Affine Layer \n",
        "    self.input_shape = [int(encoder_settings['EntityCount']),\n",
        "                        int(encoder_settings['InternalEncoderDimension'])]\n",
        "    \n",
        "    self.affine = torch.nn.Linear(input_shape[0], input_shape[1])\n",
        "    \n",
        "    # Basic GCN Layers\n",
        "    self.internal_shape = [int(encoder_settings['InternalEncoderDimension']),\n",
        "                           int(encoder_settings['InternalEncoderDimension'])]\n",
        "    \n",
        "    # Features dimension of the vertex representation is equal to the number of entities\n",
        "    vertex_feature_dimension = input_shape[0]\n",
        "    n_coefficients = int(encoder_settings['NumberOfBasisFunctions'])\n",
        "    relation_count = int(encoder_settings['RelationCount'])\n",
        "          \n",
        "    type_matrix_shape = (relation_count, n_coefficients)\n",
        "    vertex_matrix_shape = (vertex_feature_dimension, n_coefficients,  self.internal_shape[1])\n",
        "    self_matrix_shape = (vertex_feature_dimension,  self.internal_shape[1])\n",
        "\n",
        "    glorot_var_combined = glorot_variance([vertex_matrix_shape[0], vertex_matrix_shape[2]])\n",
        "        \n",
        "        \n",
        "        \n",
        "        self.W_forward = make_tf_variable(0, glorot_var_combined, vertex_matrix_shape, name=\"GCN_Basis_W_forward\")\n",
        "        self.W_backward = make_tf_variable(0, glorot_var_combined, vertex_matrix_shape, name=\"GCN_Basis_W_backward\")\n",
        "        self.W_self = make_tf_variable(0, glorot_var_combined, self_matrix_shape, name=\"GCN_Basis_W_self\")\n",
        "\n",
        "        type_init_var = 1\n",
        "        self.C_forward = make_tf_variable(0, type_init_var, type_matrix_shape, name=\"GCN_Basis_C_forward\")\n",
        "        self.C_backward = make_tf_variable(0, type_init_var, type_matrix_shape, name=\"GCN_Basis_C_backward\")\n",
        "\n",
        "        self.b = make_tf_bias(self.shape[1], name=\"GCN_Basis_bias\")\n",
        "    \n",
        "    # Relation Embedding Layer\n",
        "        \n",
        "  def forward(X):\n",
        "    \n",
        "    # Set up the Affine Layer\n",
        "    variance = glorot_variance(self.input_shape)\n",
        "    affine.weight.data.fill_(make_weight(0, variance, input_shape))\n",
        "    affine.bias.data.fill(make_bias(input_shape[1]))\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}